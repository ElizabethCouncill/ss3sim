%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{An introduction to ss3sim for stock-assessment simulation}

% Text width of 68 for the examples.
% To make version control more useful:
% Wrap the rest of the text semantically
% (e.g. at sentences and phrases)
% at approximately 80 characters maximum.
% (Much shorter lines are fine too.)
% Try not to re-wrap unless necessary.
% Avoid trailing whitespace.

\documentclass[12pt]{article}
\usepackage{geometry}
\usepackage{url}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{appendix}
\usepackage[round]{natbib}
\bibpunct{(}{)}{;}{a}{}{;}
\newcommand{\Vof}[1]{\mathrm{Var}\left[#1 \right]}
\newcommand{\Eof}[1]{\mathrm{E}\left[ #1 \right]}
\begin{document}

<<set-knitr-options, echo=FALSE>>=
library(knitr)
opts_chunk$set(fig.align='center', fig.pos="htpb", cache=TRUE, echo=TRUE,
message=FALSE, autodep=TRUE, fig.path='figure/', fig.width=5, par=TRUE)
opts_chunk$set(warning=FALSE, message=FALSE, tidy=FALSE, refresh=TRUE)
opts_chunk$set(dev = 'pdf')
@

\title{An introduction to \texttt{ss3sim} for\\stock-assessment simulation}
\author{
  Sean C. Anderson\thanks{sean@seananderson.ca; Department of Biological
  Sciences, Simon Fraser University, Burnaby BC, V5A 1S6, Canada}
  \and
  Kelli F. Johnson\thanks{School of Aquatic and Fishery Sciences,
  University of Washington, Box 355020, Seattle, WA 98195-5020, USA}
  \and
  Cole C. Monnahan\thanks{Quantitative Ecology and Resource Management,
  University of Washington, Box 352182, Seattle, WA 98195-5020, USA}
  \and
  Kotaro Ono\footnotemark[2]
  \and
  Juan L. Valero\footnotemark[2] \thanks{Center for the Advancement of
  Population Assessment Methodology, Scripps Institution of Oceanography, La
  Jolla, CA 92037, USA}
  }
\date{}
\maketitle

\clearpage
\tableofcontents

%\setlength\parskip{0.1in}
%\setlength\parindent{0in}

\clearpage
\section{Installing the \texttt{ss3sim} \texttt{R} package}

The package can be installed with:

<<install-and-load, eval=FALSE>>=
# dependencies if needed:
install.packages(c("r4ss", "plyr", "gtools", "ggplot2", "lubridate",
  "reshape2"))
# install devtools to install ss3sim directly from GitHub:
install.packages("devtools")
devtools::install_github("ss3sim", "seananderson")
@

\noindent
You can read the help files and access this vignette again with:

<<help, eval=FALSE>>=
help(package = "ss3sim")
vignette("ss3sim-vignette")
@

\noindent
We develop and test \texttt{ss3sim} around a specific version of \texttt{SS3}.
Currently, \texttt{ss3sim} is tested to work with \texttt{SS3 Version 3.24O}.
You can download \texttt{SS3} at \url{http://nft.nefsc.noaa.gov/Stock_Synthesis_3.htm}.
\texttt{ss3sim} requires the \texttt{SS3} binary to be in your path.
This means that your operating system knows where the binary file is located.
See Section \ref{sec:path} for details.

\section{An overview of the \texttt{ss3sim} simulation structure}

\texttt{ss3sim} is an \texttt{R} package that is meant to make it relatively quick and easy
to run simulations with the 3rd version of Stock Synthesis, \texttt{SS3}.
The package consists of a series of functions
that facilitate manipulating \texttt{SS3} configuration files,
running \texttt{SS3} models, and combining the output.
\texttt{ss3sim} also contains some wrapper functions
that tie all these low-level functions together
into a complete simulation experiment.
If you choose to use our wrapper function \texttt{run\_ss3sim}
then you will use a series of plain text control files
to control the simulation.
Much of this vignette focusses on
how to effectively use \texttt{run\_ss3sim},
but feel free to take the low-level functions
(\texttt{change} and \texttt{sample} functions)
and use them as part of your own flexible simulation.

\subsection{Setting up the file structure}

The \texttt{ss3sim} package is set up assuming there is
an established base-case operating model (OM) and estimation model (EM) to
work with. The \texttt{ss3sim} package comes with three generic built in
models that can be used as is, modified, or replaced.%
\footnote{See Section \ref{sec:modify} for details on these models and how to
modify them. See Appendices \ref{sec:om-setup} and \ref{sec:em-setup} for
details on creating your own OM and EMs.}
Each OM and EM should be in its own folder.
The OM folder should have the files:

\begin{verbatim}
yourOMmodel.ctl
yourOMmodel.dat
ss3.par
starter.ss
forecast.ss
\end{verbatim}

\noindent
The EM folder should have:

\begin{verbatim}
yourEMmodel.ctl
starter.ss
forecast.ss
\end{verbatim}

In both cases, nothing more and nothing less.
The names of the \texttt{.ctl} and \texttt{.dat} files are not important.
The package functions will rename them after they are copied to appropriate folders.
These files should be derived from \texttt{.ss\_new} files but with file extensions as shown above.
It's important to use \texttt{.ss\_new} files so the files have consistent formatting.
Many of the functions in this package depend on that formatting.

To obtain \texttt{.ss\_new} files,
open a command window in the OM and EM folders
and type \texttt{ss3}.
Once the model is finished running,
remove the \texttt{.ss\_new} file extension from the files needed
and add the appropriate file extension.

\subsection{Cases and scenarios}

The high-level wrapper function \texttt{run\_ss3sim}
uses unique case identifiers (IDs) that combine to create unique scenarios.
The types of cases are:
data quality (D), estimation (E), fishing mortality (F),
retrospective (R),
and any other letter describing a time varying case
(e.g.\ M for natural mortality, S for selectivity, or G for growth).
These case IDs are followed by a three-letter species identifier.
It's important to use three letters
because the functions assume that the last three letters
represent a unique identifier.
The different versions of each case are identified with numbers.
For example, the base-case scenario for a cod stock might be:
\texttt{D0-E0-F0-M0-R0-cod}.
The order of the letters doesn't matter,
as long as they are used consistently.

\texttt{ss3sim} relies on a set of plain text files to control the simulation.
These plain text files are read by \texttt{get\_caseval}
and turned into argument lists that are passed to \texttt{run\_ss3sim}.
The function \texttt{create\_argfiles} creates template input files.
It reads the various functions and parses the arguments and default values
into plain text files.
The default settings create these files:

\begin{enumerate}
\item
  \texttt{E0-spp.txt} (for estimation method cases)
\item
  \texttt{F0-spp.txt} (for fishing mortality cases)
\item
  \texttt{R0-spp.txt} (for retrospective cases)
\item
  \texttt{index0-spp.txt} (controlled by the \texttt{D} (data) case)
\item
  \texttt{agecomp0-spp.txt} (controlled by the \texttt{D} (data) case)
\item
  \texttt{lcomp0-spp.txt} (controlled by the \texttt{D} (data) case)
\item
  \texttt{X0-spp.txt} (for a time-varying parameter)
\end{enumerate}

After running \texttt{create\_argfiles()},
look in your working directory for the template files.
Change the case ID number (defaults to \texttt{0})
and the species identifier to a three letter identifier.
For example, you might use \texttt{cod}, \texttt{sar}, or \texttt{fla}
for cod, sardine, or flatfish.
An example filename would therefore be
\texttt{M1-sar.txt} or \texttt{lcomp2-fla.txt}.
The case \texttt{D1} corresponds to the files
\texttt{index1-spp.txt}, \texttt{agecomp1-spp.txt}, and\\ \texttt{lcomp0-spp.txt}.
The other case types have single argument files.

The first column in the text files
denotes the argument to be passed to a function.
The second argument denotes the value to be passed.
See the help for a \texttt{change} function
to see the arguments that need to be declared.
For example, see \texttt{?change\_f}.

You can use any simple \texttt{R} syntax to declare argument values.
For example,
\texttt{c(1, 2, 4)}, or \texttt{seq(1, 100)}.
Character objects don't need to be quoted as long as they are one word,
but can be if you'd like.
However, be careful not to use the delimiter (set up as a semicolon)
anywhere else in the file besides to denote columns.
You can add comments after any \texttt{\#} symbol.
Internally, the functions evaluate in \texttt{R}
any entries that have no character values (e.g.\ \texttt{1:100}),
or have an alpha-numeric character followed by a \texttt{(}.
Anything that is character only or has character mixed with numeric
but doesn't have the regular expression \texttt{"{[}A-Za-z0-9{]}("}
gets turned into a character argument.
(\texttt{NA} and \texttt{NULL} are special cases that are also passed on directly.)

Putting that all together,
here's what an example \texttt{F1-cod.txt} file might look like:

\begin{verbatim}
years; 1913:2012
years_alter; 1913:2012
fvals; c(rep(0, 25), rep(0.114, 75))
\end{verbatim}

\subsection{Bias adjustment}

Bias adjustment helps assure
that the estimated log-normally distributed recruitment deviations
are mean-unbiased leading to mean-unbiased estimates
of biomass \citep{methot2011}.
The high-level wrapper function \texttt{run\_ss3sim}
allows users to specify whether or not they would
like to use the bias adjustment routine built into the package
by setting \texttt{bias\_adjust} to \texttt{TRUE} or \texttt{FALSE}.
If \texttt{TRUE}, the function runs \texttt{bias\_nsim} replicates
for each scenario (located in the subfolder \texttt{bias}
within a scenario folder) and then averages
the bias-adjustment parameter estimates.
If a bias adjustment run fails to converge,
or the Hessian is not invertible,
the parameter estimates from that run are ignored.
A minimum threshold of 80\% converged bias adjustment runs
is used to ensure reliable parameter estimates.
The mean bias adjustment runs are then used in the EM
for all subsequent ``iterations''
(i.e. replicates using same models but different process and observation errors)
within that scenario.
We assume bias adjustment parameters applicable
to all iterations within the scenario are more likely
to be found by averaging across multiple sets of parameters.
The bias adjustment process creates several files in the \texttt{bias} folder
(for each scenario) which can be examained by the user.  \texttt{AdjustBias.DAT}
contains the calculated adjustment parameters for each bias run, and
\texttt{AvgBias.DAT} contains the average of these estimates which are used in
all subsequent simulations for that scenario. Individual plots for each
successful bias adjustment run are also created.
If \texttt{bias\_adjust} is set to \texttt{FALSE},
no bias adjustment runs are executed and no bias adjustment is performed.

\subsection{Output file structure}

Internally, the function \texttt{copy\_ss3models} creates a folder structure
and copies the operating and estimation models.
The folder structure looks like:

\begin{verbatim}
  D0-E0-F0-M0-R0-cod/1/om
  D0-E0-F0-M0-R0-cod/1/em
  D0-E0-F0-M0-R0-cod/2/om
  D0-E0-F0-M0-R0-cod/2/em
  ...
\end{verbatim}

The integer values after the scenario ID are the different iterations, the
number of which are run is specified by the user. If you are using bias
adjustment (\texttt{bias\_adjust = TRUE}) then there will be some additional
folders.  In that case the folders will look like:

\begin{verbatim}
  D0-E0-F0-M0-R0-cod/bias/1/om
  D0-E0-F0-M0-R0-cod/bias/1/em
  D0-E0-F0-M0-R0-cod/bias/2/om
  D0-E0-F0-M0-R0-cod/bias/2/em
  ...
  D0-E0-F0-M0-R0-cod/1/om
  D0-E0-F0-M0-R0-cod/1/em
  D0-E0-F0-M0-R0-cod/2/om
  D0-E0-F0-M0-R0-cod/2/em
  ...
\end{verbatim}

Note that the OM and EM folders will be renamed
\texttt{om} and \texttt{em} within each iteration,
the OM and EM are checked
to make sure they contain the minimal files (as listed above),
the filenames will be all lowercase,
the data file is renamed \texttt{ss3.dat},
the control files are renamed \texttt{om.ctl} or \texttt{em.ctl},
and the starter and control files are adjusted
to reflect these new file names.
The functions in this package
assume you've set your working directory in \texttt{R}
to be the base folder where you will store the scenario folders.

\section{An example simulation with \texttt{ss3sim}}

As an example, we will run a 2x2 simulation design in which we test
(1) the effect of high and low research survey effort and
(2) the effect of fixing versus estimating natural mortality (\textit{M}).
All of the files for this example are contained
within the \texttt{ss3sim} package.
To start, we'll locate three sets of folders:
the folder with the plain text case files,
the folder with the OM,
and the folder with the EM.

<<locate-folders>>=
library(ss3sim)
d <- system.file("extdata", package = "ss3sim")
case_folder <- paste0(d, "/eg-cases")
om <- paste0(d, "/models/cod-om")
em <- paste0(d, "/models/cod-em")
@

See the folder \texttt{ss3sim/inst/extdata/eg-cases}
inside the \texttt{ss3sim} source code
for all the case files that are used in this example simulation.
You can either download the source code
or find this folder at the location contained
in the \texttt{case\_folder} object defined
in the previous chunk of \texttt{R} code.

\subsection{Creating the case files}

We will base the simulation around the base-case files created
for a cod-like species in the papers
\citet{ono2013} and \citet{johnson2013}.
You can refer to these papers for details on how the models were set up.

If we were starting from scratch,
we would run the function \texttt{create\_argfiles()},
which would create a default set of configuration files
in our \texttt{R} working directory.
Instead we'll start with the configuration files from those papers.

To investigate the effect of research survey effort,
we will manipulate the argument \texttt{sds\_obs}
that gets passed to \texttt{change\_index}.
This argument ultimately refers to the standard error
on the log(index) value, as defined in \texttt{SS3}.
In case 0, we'll specify the standard deviation at 0.1
and in case 1 we'll increase the standard deviation to 0.4.
We can do this by including the line: \texttt{sds\_obs; 0.1}
in the file \texttt{D0-cod.txt} and the line: \texttt{sds\_obs; 0.4}
in the file \texttt{D1-cod.txt}.

The file \texttt{index0-cod.txt} will therefore look like:

\begin{verbatim}
fleets;  2
years;   list(seq(1974, 2012, by=2))
sds_obs; list(0.1)
\end{verbatim}

\noindent
\texttt{fleets} refers to the fleet number we want to sample from (as defined in
the model), \texttt{2} in
\texttt{codOM.dat} refers to the survey fleet. We then run our survey from
1974 to 2012 with a standard error on the log(survey index) of 0.1.

We won't describe the length or age composition sampling here in detail, but
you can refer to the help files \texttt{?change\_lcomp} and
\texttt{?change\_agecomp} along with the case files included in the package
data. Also see Section \ref{sec:comps} where we describe the theory behind the
age- and length-composition sampling in \texttt{ss3sim}.

To investigate the effect of fixing versus estimating \textit{M},
we'll manipulate the argument \texttt{natM\_val} that gets passed to \texttt{change\_e}.
The first entry of \texttt{natM\_val} is the value which \textit{M} is fixed or
initialized at and the second refers the phase.
In case 0, we'll set the phase in which \textit{M} is estimated
to \texttt{-1} (any negative number will work) and use the argument \texttt{NA}
to fix \textit{M} at the true value (i.e. do not modify it from what is in the
OM).
In case 1, we'll estimate \textit{M} in
phase \texttt{3} and initialize the estimation of \textit{M} at \texttt{0.20}.
We can do this by including the line: \texttt{natM\_val; c(NA, -1)}
in the file \texttt{E0-cod.txt}
and the line: \texttt{natM\_val; c(0.20, 3)} in the file \texttt{E1-cod.txt}.
This means that case \texttt{E1} will initialize \textit{M} at \texttt{0.20}
and estimate \textit{M} in the third phase.
For example, the complete case file \texttt{E0-cod.txt} is:

\begin{verbatim}
natM_type;          1Parm
natM_n_breakpoints; NULL
natM_lorenzen;      NULL
natM_val;           c(NA,-1)
par_name;           LnQ_base_3_CPUE
par_int;            NA
par_phase;          -1
forecast_num;       0
\end{verbatim}

Since we won't be running a retrospective analysis
in this example, this will be our \texttt{R0-cod.txt} file:

\begin{verbatim}
retro_yr; 0
\end{verbatim}

We'll set the fishing mortality $F$ to a constant level
at $F_\mathrm{MSY}$ ($F$ at maximum sustainable yield)
from when the fishery starts (at 25 years)
until the end of our simulation (100 years).
Therefore, this is our \texttt{F0-cod.txt}:

\begin{verbatim}
years;       1913:2012
years_alter; 1913:2012
fvals;       c(rep(0,25), rep(0.114,75))
\end{verbatim}

Although we don't have any time-varying processes in our example simulation,
we include \texttt{M} case files
in which we describe \textit{M} as stationary at a constant value (\texttt{M0-cod.txt}):

\begin{verbatim}
function_type; change_tv
param;         NatM_p_1_Fem_GP_1
dev;           rep(0, 100)
\end{verbatim}

Simply by changing the deviations from a vector of zeros,
we could add time-varying \textit{M}.
In Section \ref{sec:time-varying} we describe how time-varying parameters are unique in \texttt{ss3sim} and how you can make any parameter
described in \texttt{SS3} time varying using this approach.

\subsection{Checking the case files}

It's a good idea to check that the case files
are manipulating the \texttt{SS3} model files as we intended.
One way we can do this is by
running the \texttt{change} functions directly on the SS3 model files
using the arguments specified within the case files
and inspecting the modified \texttt{SS3} files.
Another way is to run a single iteration of your simulation
through \texttt{run\_ss3sim}
and carefully inspect the OM and EM model files that it creates.
For example, we could run:

<<case-file-checks, eval=FALSE>>=
run_ss3sim(iterations = 1, scenarios =
  c("D0-E0-F0-R0-M0-cod",
    "D1-E0-F0-R0-M0-cod",
    "D0-E1-F0-R0-M0-cod",
    "D1-E1-F0-R0-M0-cod"),
  case_folder = case_folder, om_model_dir = om,
  em_model_dir = em)
@

\noindent
And then check the \texttt{SS3} files that are created.

\subsection{Running deterministic simulations to check the models for
bias}
\label{sec:deterministic}

We'll run some simulations to check our model for bias when process error is not
included in the OM.
To do this, we'll start by setting up a 100 row (number of years) by 20 column
(number of iterations) matrix of recruitment deviations, where all values are
set to zero.

<<>>=
recdevs_det <- matrix(0, nrow = 100, ncol = 20)
@

Then we'll set up case ``estimation'' files in which the
initialized values of the recruitment deviation
standard deviations, \texttt{SR\_sigmaR},
are set to the nominal level of \texttt{0.001}.
We'll name these files \texttt{E100-cod.txt} and \texttt{E101-cod.txt}.
In the case files, the key element is setting
\texttt{par\_name = SR\_sigmaR} (the SS parameter name) and \texttt{par\_int = 0.001} (the initial value).
The arguments \texttt{par\_name} and \texttt{par\_int} are set up
to handle vectors of parameter names and values.
In this example, changing
\texttt{SR\_sigmaR} will be the second parameter in the vector.
Again, you can look at all the case files in the package folder \texttt{/inst/extdata/eg-cases/}.
As an example, here is the file \texttt{E101-cod}:

\begin{verbatim}
natM_type;          1Parm
natM_n_breakpoints; NULL
natM_lorenzen;      NULL
natM_val;           c(0.20,3)
par_name;           LnQ_base_3_CPUE,SR_sigmaR
par_int;            c(NA,0.001)
par_phase;          c(-1,-1)
forecast_num;       0
\end{verbatim}

When we run the simulations,
we'll pass our deterministic recruitment deviations
to the function \texttt{run\_ss3sim}.
Running 20 iterations should be enough
to identify whether our models are performing as we expect.
Note that by default 10 bias adjustment runs are performed since we specify that
\texttt{bias_adjust=TRUE}.

<<deterministic-runs, eval=FALSE>>=
run_ss3sim(iterations = 1:20, scenarios =
  c("D0-E100-F0-R0-M0-cod",
    "D1-E100-F0-R0-M0-cod",
    "D0-E101-F0-R0-M0-cod",
    "D1-E101-F0-R0-M0-cod"),
  case_folder = case_folder, om_model_dir = om, em_model_dir = em,
  bias_adjust = TRUE, user_recdevs = recdevs_det)
@

We have written out the scenario names in full for clarity,
but \texttt{ss3sim} also contains
a convenience function \texttt{expand\_scenarios}.
With this function we could instead write:

<<deterministic-runs-expand, eval=FALSE>>=
x <- expand_scenarios(list(D = 0:1, E = 100:101, F = 0, M = 0,
    R = 0), species = "cod")
run_ss3sim(iterations = 1:20, scenarios = x,
  case_folder = case_folder, om_model_dir = om, em_model_dir = em,
  bias_adjust = TRUE, user_recdevs = recdevs_det)
@

Note that due to the way that \texttt{SS3} is being used as an operating model,
you may see an ADMB error in the console:
\begin{verbatim}
Error -- base = 0 in function prevariable& pow(const prevariable& v1, CGNU_DOUBLE u)
\end{verbatim}
\noindent
However, this is not a problem since ADMB is not used to optimize the OM and the
error can safely be ignored.
\subsection{Running the stochastic simulations}

The package contains a set of normally-distributed recruitment deviations,
with a mean of \texttt{-0.5} and a standard deviation of \texttt{1}
(bias-corrected standard-normal deviations).
To use the pre-specified deviations
remove the argument \texttt{user\_recdevs} from \texttt{run\_sssim}.
Process error will now be unique for each iteration but consistent across scenarios
for a specific iteration, to facilitate comparison between scenarios.

We'll only run 50 iterations here to save time and keep the package size
down, but you would likely want to run many more iterations ($\ge 100$), perhaps testing
how many iterations are required before the key results stabilize.

<<stochastic-runs, eval=FALSE>>=
run_ss3sim(iterations = 1:100, scenarios =
  c("D0-E0-F0-R0-M0-cod",
    "D1-E0-F0-R0-M0-cod",
    "D0-E1-F0-R0-M0-cod",
    "D1-E1-F0-R0-M0-cod"),
  case_folder = case_folder, om_model_dir = om,
  em_model_dir = em, bias_adjust = TRUE)
@

\subsection{Reading in the output and plotting the data}

The function \texttt{get\_results\_all} reads in a set of scenarios
and combines the output into two \texttt{.csv} files:
\texttt{ss3sim\_scalar.csv} and \texttt{ss3sim\_ts.csv}. The ``scalar'' file
contains values for which there is a single value (e.g.\ MSY) and the ``ts''
file refers to values for which there are time series available (e.g.\ biomass
for each year). The column names refer to the output from \texttt{SS3}, so it
can be useful to consult the \texttt{SS3} manual for details.

<<get-results, eval=FALSE>>=
get_results_all(user_scenarios =
  c("D0-E100-F0-R0-M0-cod",
    "D1-E100-F0-R0-M0-cod",
    "D0-E101-F0-R0-M0-cod",
    "D1-E101-F0-R0-M0-cod",
    "D0-E0-F0-R0-M0-cod",
    "D1-E0-F0-R0-M0-cod",
    "D0-E1-F0-R0-M0-cod",
    "D1-E1-F0-R0-M0-cod"))
@

\noindent
Let's read in the \texttt{.csv} files:

<<read-output, eval=FALSE>>=
scalar_dat <- read.csv("ss3sim_scalar.csv")
ts_dat <- read.csv("ss3sim_ts.csv")
@

\noindent
Or if you'd like to follow along with the rest of the vignette
without running the simulations above,
you can load a saved version of the output:

<<load-output>>=
load("ts_dat.rda")
load("scalar_dat.rda")
@

\noindent
First, we'll calculate some useful values in new columns and separate the
deterministic from the stochastic simulation runs:

<<transform-output>>=
scalar_dat <- transform(scalar_dat,
  steep = (SR_BH_steep_om - SR_BH_steep_em)/SR_BH_steep_om,
  logR0 = (SR_LN_R0_om - SR_LN_R0_em)/SR_LN_R0_om,
  depletion = (depletion_om - depletion_em)/depletion_om,
  SSB_MSY = (SSB_MSY_em - SSB_MSY_om)/SSB_MSY_om,
  SR_sigmaR = (SR_sigmaR_em - SR_sigmaR_om)/SR_sigmaR_om,
  NatM =
    (NatM_p_1_Fem_GP_1_em - NatM_p_1_Fem_GP_1_om)/
     NatM_p_1_Fem_GP_1_om)

ts_dat <- transform(ts_dat,
  SpawnBio = (SpawnBio_em - SpawnBio_om)/SpawnBio_om,
  Recruit_0 = (Recruit_0_em - Recruit_0_om)/Recruit_0_om)
ts_dat <- merge(ts_dat, scalar_dat[,c("scenario", "replicate",
    "max_grad")])

scalar_dat_det <- subset(scalar_dat, E %in% c("E100", "E101"))
scalar_dat_sto <- subset(scalar_dat, E %in% c("E0", "E1"))
ts_dat_det <- subset(ts_dat, E %in% c("E100", "E101"))
ts_dat_sto <- subset(ts_dat, E %in% c("E0", "E1"))
@

\noindent
Now we'll turn the scalar data into long-data format so we can make a
multipanel plot with \texttt{ggplot2}.

<<reshape-scalars>>=
scalar_dat_long <- reshape2::melt(scalar_dat[,c("scenario", "D", "E",
  "replicate", "max_grad", "steep", "logR0", "depletion", "SSB_MSY",
  "SR_sigmaR", "NatM")], id.vars = c("scenario", "D", "E",
  "replicate", "max_grad"))
scalar_dat_long <- plyr::rename(scalar_dat_long,
  c("value" = "relative_error"))
@

\noindent
Now let's look at boxplots of the deterministic model runs.

<<relative-error-boxplots-det, fig.height=7, fig.width=5, out.width="4in", cache=TRUE, fig.cap="Relative error box plots for deterministic runs. In case E100, \\textit{M} is fixed at the historical value; in E101 we estimate \\textit{M}. In case D2, the standard deviation on the survey index observation error is 0.4. In case D1, the standard deviation is quartered representing an increase in survey sampling effort.">>=
library(ggplot2)
p <- ggplot(subset(scalar_dat_long, E %in% c("E100", "E101") &
       variable != "SR_sigmaR"), aes(D, relative_error)) +
     geom_boxplot() +
     geom_hline(aes(yintercept = 0), lty = 2) +
     facet_grid(variable~E) +
     geom_jitter(aes(colour = max_grad),
     position = position_jitter(height = 0, width = 0.1),
       alpha = 0.4, size = 1.5) +
     scale_color_gradient(low = "darkgrey", high = "red") +
     theme_bw()
print(p)
@

\noindent
Let's look at the relative error in estimates of spawning biomass.
We'll colour the time series according to the maximum gradient.
Small values of the maximum gradient (approximately 0.001 or less)
indicate that convergence is likely.
Larger values (greater than 1) indicate that convergence is unlikely.

<<plot-sto-ts, fig.height=5, fig.width=7, fig.cap="Time series of relative error in spawning stock biomass.">>=
library(ggplot2)
p <- ggplot(ts_dat_sto, aes(x = year)) + xlab("Year") +
    theme_bw() + geom_line(aes(y = SpawnBio, group = replicate,
    colour = max_grad), alpha = 0.3, width = 0.15) + facet_grid(D~E) +
    scale_color_gradient(low = "gray", high = "red")
print(p)
@

<<ssb-ts-plots, fig.height=5, fig.width=7, cache=TRUE, fig.cap="Spawning stock biomass time series.">>=
p <- ggplot(ts_dat_sto, aes(year, SpawnBio_em, group = replicate)) +
  geom_line(alpha = 0.3, aes(colour = max_grad)) + facet_grid(D~E) +
  scale_color_gradient(low = "darkgrey", high = "red") + theme_bw()
print(p)
@

<<relative-error-boxplots-sto, fig.height=7, fig.width=5, out.width="4in", cache=TRUE, fig.cap="Relative error box plots for stochastic runs. In case E0, \\textit{M} is fixed at the historical value; in E1 we estimate \\textit{M}. In case D2, the standard deviation on the survey index observation error is 0.4. In case D1, the standard deviation is quartered representing an increase in survey sampling effort.">>=
p <- ggplot(subset(scalar_dat_long, E %in% c("E0", "E1")),
       aes(D, relative_error)) +
     geom_boxplot() + geom_hline(aes(yintercept = 0), lty = 2) +
     facet_grid(variable~E) +
     geom_jitter(aes(colour = max_grad),
       position = position_jitter(height = 0, width = 0.1),
       alpha = 0.4, size = 1.5) +
     scale_color_gradient(low = "darkgrey", high = "red") +
     theme_bw()
print(p)
@

\clearpage

\subsection{Time varying parameters}
\label{sec:time-varying}

You can specify any parameter defined in your \texttt{SS3} model
as time varying through the \texttt{change\_tv} function.
The function works by adding an environmental deviate ($env$)
to the base parameter ($par$),
creating a time varying parameter ($par'$) for each year ($y$),
\begin{equation}
par'_y = par + link * env_y.
\end{equation}
\noindent
The $link$ is pre-specified to a value of one and
$par$ is the base value for the given parameter,
as defined in the \texttt{.par} file.
For all catchability parameters (\textit{q}), the deviate will be added to the
log transform of the base parameter using the following equation:
\begin{equation}
  \log(q'_{y}) = \log(q) + link * env_{y}.
\end{equation}
\noindent
The vector of deviates must contain one entry for every year of the simulation
and can be specified as zero for years in which the parameter does not deviate.

Currently, the function only works to add time varying properties
to a time invariant parameter.
It cannot alter the properties of parameters that already vary with time.
Additionally, SS3 does not allow more than one stock recruit parameter
to vary with time. Therefore, if the \texttt{.ctl} file already has a
stock recruit parameter that varies with time and you try to implement another,
the function will fail.

Since the \texttt{change\_tv} function can be used for a number of
purposes, it interacts differently with the case files than the other
\texttt{change} functions.
To pass arguments to \texttt{change\_tv} through
\texttt{run\_ss3sim}: (1) create a case file with an arbitrary letter
not used elsewhere (i.e.\ anything but D, E, F, or R) and (2) include the line
\begin{verbatim}
function_type; change_tv
\end{verbatim}
\noindent
in your case file. For example,
you might want to use M for natural mortality, S for selectivity, or G
for growth.

\section{Modifying the included operating and estimation models}
\label{sec:modify}
\texttt{ss3sim} comes with three built-in \texttt{SS3} operating and estimation
models: a cod-like (slow-growing and long-lived), flatfish-like (fast-growing
and intermediate-lived), and sardine-like (fast-growing and short-lived).  These
models are based on North Sea cod (\textit{Gadus morhua}) (R. Methot, pers.\
comm.), yellowtail flounder (\textit{Limanda ferruginea}) (R. Methot, pers.\
comm.), and California sardine (\textit{Sardinops caeruleus}) \citep{hill2012}.
Further details on these models are available in \citep{johnson2013} and
\citep{ono2013}. These models were stripped down and simplified to make them
more generic for simulation testing. In doing this, we removed many of the
subtle features of the model.
While these models are generic and cover a wide range
of life history types, they may not be suitable for all users. Therefore, in this section,
we outline strategies for modifying the existing \texttt{SS3} models.

Before proceeding it is worth considering the scope and
place of \texttt{ss3sim} as a simulation package. The package was designed as a
vehicle for examining structural differences in models: between an OM and EM
\citep[e.g.][]{johnson2013} or between multiple EMs \citep[e.g.][]{ono2013}. Therefore,
the specific details (e.g.\ many fleets, tagging data, seasons, etc.) of the
models were not important and removed to produce a set of generic life history
type models. \texttt{ss3sim} is not designed for testing
arbitrary \texttt{SS3} models, but rather properties of assessment models in
general. Thus \texttt{ss3sim} is not ideal for quickly exploring properties of a
particular assessment model and other software packages should be explored if
that is your goal.

Here is a list of \texttt{SS3} features that are not currently implemented in \texttt{ss3sim}:
\begin{itemize}
\item Seasons, sexes, hermaphrodism, area, movement
\item Data other than suverys and age/length compositions: age-at-length,
  discards, etc.
\end{itemize}
Some of the features may work within the \texttt{ss3sim} framework, but are untested.

It is possible to create new models that will work within the \texttt{ss3sim}
framework, but this task will be complicated and likely require extensive
knowledge of \texttt{SS3} and R, as well as modification of the \texttt{ss3sim}
functions. This process is described in more detail in Appendices
\ref{sec:om-setup} and \ref{sec:em-setup}.

Instead of creating entirely new models, we recommend adapting the current
built-in models to match the desired properties for a simulation study. Since
these models have been thoroughly tested and used with \texttt{ss3sim} already,
they make an ideal starting place. Before proceeding it would be wise to examine
the built-in models to determine how closely they match your desired model and
whether simple changes can get you reasonably close for simulation.

Say for example you want to modify the cod model to have different
maturity, and then explore different sampling schemes using the
\texttt{change\_index}, \texttt{change\_lcomps}, and \texttt{change\_agecomps}
functions. The follow steps provide a basic guideline for how to accomplish this:
\begin{itemize}
\item Using the original cod model create the case argument files you desire for
  your simulation and verify they run with the original cod model using the
  function \texttt{run\_ss3sim}. It is probably best to do a shorter
  deterministic (Section \ref{sec:deterministic}) run. After running, read in the
  data and do visual checks for proper functionality.
\item Find the original cod model. This can be found in the
  \texttt{inst/extdata/models} folder inside the package, located by
\begin{verbatim}
library(ss3sim)
d <- system.file("extdata", package = "ss3sim")
paste0(d, "/models")
\end{verbatim}
  Make a copy of the cod models (OM and EM) and rename them as desired.
\item Make a single change to either the \texttt{.dat} or \texttt{.ctl} files
  for the new model and run them manually with \texttt{SS3}
  if there is any question if they might break the model.
\item Rerun the model through \texttt{run\_ss3sim} and verify it is still
  working. If errors occur in the
  \texttt{R} function you will need to examine the function to determine why the error is
  occurring and fix by changing the \texttt{R} function and reloading it.
\item Repeat previous steps with all small changes until the models are
  satisfactory.
\item Turn off deterministic runs and run the simulation stochastically.
\item If your new model works well with the package and is significantly
  different than what is built-in, please contact the \texttt{ss3sim} package
  managers for inclusion in future versions.
\end{itemize}


\section{Using \texttt{ss3sim\_base} directly}

If you'd prefer, you can skip
setting up the case files
and use \texttt{ss3sim\_base} directly
by passing lists of arguments.
The lists correspond to the arguments
to each of the \texttt{change} functions.
In this case,
the scenario ID only serves the function
of identifying the output folder name
and could technically be any character string you'd like.

For example,
we could have run the scenario \texttt{D1-E0-F0-R0-M0-cod}
that we ran before like this:

<<ss3sim-base-eg, eval=FALSE>>=
ss3sim_base(iterations = 1:20, scenarios = "D1-E0-F0-R0-M0-cod",
  f_params = list(years = 1913:2012, years_alter = 1913:2012, fvals
    = c(rep(0, 25), rep(0.114, 75))),
  index_params = list(fleets = 2, years = list(1974:2012), sds_obs =
    list(0.1)),
  lcomp_params = list(fleets = c(1, 2), Nsamp = list(100, 100),
    years = list(1938:2012, seq(1974, 2012, by = 2)),
    lengthbin_vector = NULL, cpar = c(1, 1)),
  agecomp_params = list(fleets = c(1, 2), Nsamp = list(100, 100),
    years = list(1938:2012, seq(1974, 2012, by = 2)),
    agebin_vector = NULL, cpar = c(1, 1)),
  estim_params = list(natM_type = "1Parm", natM_n_breakpoints =
    NULL, natM_lorenzen = NULL, natM_val = c(NA,-1), par_name =
    "LnQ_base_3_CPUE", par_int = NA, par_phase = -1, forecast_num =
    0),
  tv_params = list(NatM_p_1_Fem_GP_1 = rep(0, 100)),
  retro_params = list(retro_yr = 0),
  om_model_dir = om,
  em_model_dir = em)
@

\section{Parallel computing with \texttt{ss3sim}}
\label{sec:parallel}

\texttt{ss3sim} can easily run multiple scenarios in parallel
to speed up simulations.
To run a simulation in parallel,
you need to register multiple cores or clusters and
set \texttt{parallel = TRUE} in \texttt{run\_ss3sim}.
For example, we could have run the previous example in parallel with the following code.
First, we'll register four cores:

<<parallel-one>>=
require(doParallel)
registerDoParallel(cores = 4)
@

\noindent
We can check to make sure we're set up to run in parallel:

<<parallel-two>>=
require(foreach)
getDoParWorkers()
@

\noindent
And then run our simulation on four cores simultaneously
by setting \texttt{parallel = TRUE}:

<<parallel-three, eval=FALSE>>=
run_ss3sim(iterations = 1:100, scenarios =
  c("D1-E0-F0-R0-M0-cod",
    "D2-E0-F0-R0-M0-cod",
    "D1-E1-F0-R0-M0-cod",
    "D2-E1-F0-R0-M0-cod"),
  case_folder = case_folder, om_model_dir = om, em_model_dir = em,
  bias_adjust = TRUE, parallel = TRUE)
@

\noindent
In addition to the check with \texttt{getDoParWorkers()} above,
if the simulations are running in parallel,
you will also see simultaneous output messages
from \texttt{ss3sim} as the simulations run.
On a 2.27 GHz Intel Xeon quad-core server running Ubuntu 10.04,
this example ran 1.9 times faster on two cores than on a single core,
and 3.2 times faster on four cores.


\section{Putting \texttt{SS3} in your path}
\label{sec:path}

Instead of copying the \texttt{SS3} model file (\texttt{ss3.exe}) to each folder
within a simulation and running it, \texttt{ss3sim} relies on a single binary file
being available to the operating system regardless of the folder.
To accomplish this, \texttt{SS3} must be in your system path, which is
a list of folders that your operating system looks in
whenever you type the name of a program on the command line.

\subsection{For Unix (OS X and Linux)}

To check if \texttt{SS3} is in your path:
open a Terminal window and type \texttt{which SS3} and hit enter.
If you get nothing returned, then SS is not in your path.
The easiest way to fix this is to move the \texttt{SS3} binary
to a folder that's already in your path.
To find existing path folders type \texttt{echo \$PATH}
in the terminal and hit enter.
Now move the \texttt{SS3} binary to one of these folders.
For example, in a Terminal window type:

\begin{verbatim}
sudo cp ~/Downloads/SS3 /usr/bin/
\end{verbatim}

You will need to use \texttt{sudo}
and enter your password after to have permission
to move a file to a folder like \texttt{/usr/bin/}.
If you've previously modified your path to add a non-standard location
for the \texttt{SS3} binary,
you may need to also tell \texttt{R} about the new path.
The path that \texttt{R} sees may not include additional paths
that you've added through a configuration file like \texttt{.bash\_profile}.
You can add to the path that \texttt{R} sees
by including a line like this in your \texttt{.Rprofile} file.
(This is an invisible file in your home directory.)

\begin{verbatim}
Sys.setenv(PATH=paste(Sys.getenv("PATH"),"/my/folder",sep=":"))
\end{verbatim}

\subsection{For Windows}

To check if SS is in your path for Windows 7 and 8:
open a DOS prompt and type \texttt{ss3 -?} and hit enter.
If you get a line like \texttt{ss3 is not recognized...}
then SS is not in your path.
To add it to your path:

\begin{enumerate}
\item
  Find the correct version of the \texttt{ss3.exe} binary on your
  computer
\item
  Record the folder location. E.g.\ \texttt{C:/SS3.24o/}
\item
  Click on the start menu and type \texttt{environment}
\item
  Choose \texttt{Edit environment variables for your account}
  under Control Panel
\item
  Click on \texttt{PATH} if it exists, create it if doesn't exist
\item
  Choose \texttt{PATH} and click edit
\item
  In the \texttt{Edit User Variable} window
  add to the \textbf{end} of the \texttt{Variable value} section
  a semicolon and the \texttt{SS3} folder location you recorded earlier.
  E.g.\ \texttt{;C:/SS3.24o/}. \textbf{Do not overwrite what was previously in
    the PATH variable}.
\item
  Restart your computer
\item
  Go back to the DOS prompt
  and try typing \texttt{ss3 -?} and hitting return again.
\end{enumerate}


\section{Incorporating observation error}
\label{sec:obs-error}
Observation error (i.e.\ uncertainty arising from sampling the
``truth'') can be added within the \texttt{ss3sim} framework in three
places: indices, length compositions, and age compositions. There are
functions which take the expected values produced in the OM
\texttt{.dat} files and samples from them to create the input
\texttt{.dat} files for the EM. Since this sampling process is done
dynamically with functions there is a lot of flexibility that can be
added by the user.

This section briefly details the background and functionality of these
functions.

\begin{description}
\item[\texttt{change\_index}] This function samples from the biomass
  trends for the different fleets to simulate the collection of CPUE
  and survery index data. Since $q=1$ for both the OM and EM, the CPUE
  indices are actually absolute indices of biomass. The result of the
  random sampling is a log-normal observed CPUE that is centered at
  the expected value. The user can specify which fleets are sampled in
  which years and with what amount of noise. Note that the
  years/fleets specified to be sampled must be present in the OM file,
  so the user must setup the OM accordingly.
\item[\texttt{change\_lcomp}] This function samples from the length
  composition using a Dirichlet distribution. The Dirichlet
  distribution can be thought of as a generalization of the
  multinomial distribution that has an additional parameter
  controlling the variance of the random deviates (see Section
  \ref{sec:comps}). The user can specify which fleets are sampled in
  which years and with what amount of noise. Further, the bin
  structure can be inputted as an argument and thus changed
  dynamically within a simulation (see \ref{sec:dybin} below for more
  information).
\item[\texttt{change\_agecomp}] Same as length compositions except
  acts on ages.
\end{description}

\subsection{Sampling age and length compositions}
\label{sec:comps}
[fixme: do we have these refs? Maybe Kot found some for his paper?]
When simulating assessment models we need a way to mimic the way that
the age/length compositions (comps) are sampled in reality.  Under
perfect mixing of fish and truly random sampling the distribution of
samples would be multinomial (REF).  However, in practice neither of
these are true because the fish tend to aggregate by size and age, and
it is difficult to take random samples.  This causes the data to have
more variance than expected, i.e.\ be overdispersed (REF).  If a
multinomial likelihood is assumed and the true sample size is used
then in effect the model puts too much weight on the composition data,
at the cost of fitting other sources of data less well (REF).
Analysts thus need to ``tune'' their model to find a more appropriate
sample size that more accurately reflects the information in the comp
data.

In simulation work, the true underlying proportions
in the population are known and true multinomial sampling is possible.
There is some question about the realism
behind providing the model with this kind of data,
since it never happens in practice (REF).
Thus, there is a need to generate more realistic data.
In this package we use the Dirichlet distribution
to generate such data for a specified level of overdispersion, instead
of manipulating the effective sample size.

\subsubsection{Multinomial vs.\ Dirichlet distributions}
The following calculations are shown for age compositions
but apply equally to length compositions.
The multinomial distribution $\mathbf{m} \sim
\text{MN}\left(\mathbf{p}, n, A\right )$ is defined as
\begin{align*} \mathbf{m} &= m_1, \ldots, m_A\\
  &=\text{the number of fish observed in bin $a$}\\
  \mathbf{p}&= p_1, \ldots, p_A\\
  &=\text{the true proportion of fish in bin $a$}\\
  n&=\text{sample size} \\
  A&=\text{number of age bins}
\end{align*}
In the case of Stock Synthesis,
actual proportions are input as data
so $\mathbf{m}/n$ is the distribution used instead of $\mathbf{m}$.
Thus, the variance of the estimated proportion for age bin $a$
is $\Vof{M_a/n}=p_aq_a/n$.
Note that $m_a/n$ can only take on values in the set $\{0, 1/n, 2/n, \ldots, 1\}$.
With sufficiently large sample size this set approximates the real interval $[0,1]$,
but the point being that only some values of $m_a/n$ are possible.

The Dirichlet distribution $\mathbf{d} \sim
\text{Dirichlet}\left (\boldsymbol{\alpha}, A\right )$ is
\begin{align*}
  \mathbf{d} &= d_1, \ldots, d_A\\
  &=\text{the proportion of fish observed in bin $a$}\\
  \boldsymbol{\alpha}&= \alpha_1, \ldots, \alpha_A\\
  &=\text{concentration parameters for the proportion of fish in bin $a$}\\
  A&=\text{number of age bins}
\end{align*}

Since we are using the Dirichlet distribution to generate random samples,
it is convenient to parameterize the vector of concentration parameters
$\boldsymbol{\alpha}$ as $\lambda \mathbf{p}$
so that $\alpha_a=\lambda p_a$.
The mean of $d_a$ is then $\Eof{d_a}=p_a$ and the variance is
$\Vof{d_a}=\frac{p_aq_a}{\lambda+1}$.
The marginal distributions for the Dirichlet are beta distributed.
In contrast to the multinomial above,
the Dirichlet generates points on the real interval $[0,1]$.

\subsubsection{Sampling with overdispersion}
It is clear that $\lambda$ controls the amount of overdispersion
in the generated samples: as it increases the variance decreases.
Thus, it is simple to calculate what $\lambda$ should be
in order to have a specific level of variance in the samples.
We know that the variance of the Dirichlet samples
should never be smaller than that of a multinomial (i.e.\ under-dispersed)
so it makes sense to use the multinomial variance
(with a particular sample size $n$)
as a baseline.

The following steps are used to generate overdispersed samples:
\begin{enumerate}
\item Get the true proportions at age
  (from the operating model ``truth'')
  for the number of age bins $A$.
\item Determine a realistic sample size, say $n=100$.
  Calculate the variance of the samples
  from a multinomial distribution, call it $V_{m_a}$.
\item Specify a level, $c$, that scales the standard deviation
  of the multinomial.
  Then $\sqrt{V_{d_{a}}}=c\sqrt{V_{m_{a}}}$
  from which $\lambda=n/c^2-1$ can be solved.
  Samples from the Dirichlet with this value of $\lambda$
  will then give the appropriate level of variance.
  For instance, we can generate samples
  with twice the standard deviation of the multinomial by setting $c=2$.
\end{enumerate}

The composition sampling functions provided in the package allows you
to specify the level of overdispersion.  Note that the package does
not allow for tuning of the effective sample size, and the argument
\texttt{cpar} controls the level of overdispersion in the sampling
functions.  See the function documentation for
\texttt{change\_agecomp} and \texttt{change\_lcomp} for more detail.

\subsubsection{Dynamic composition binning} \label{sec:dybin}

The functions \texttt{change\_lcomp} and \texttt{change\_agecomp}
include optional arguments specifying the bins to use. The user may
specify a vector of values determining how to bin the data.  This
option may be useful for users who wish to explore the impact of
different binning schemes of the same data. The function works by
determining which OM bins to collapse and summing across these to form
the new bins. Thus the specified bins need to coincide (i.e.\ be a
subset) with the OM bins. For instance, of the OM bins are
$b_1=(0,1,2,3,4,5)$ then this could be collapsed into the bins
$b_2=(0-1, 2-3, 4-5)$. Because of the way the function is written it
is paramount that the bins given by the user be compatible with the OM
bins. For instance, it is not possible (within the framework provided
here) to include bins outside of the range of the OM bins. We have
provided basic checks to the validity of the user provided bin
structure. However, \textbf{we stress that dynamic binning is a
  complicated process and the user is responsible for thoroughly
  testing their proposed binning structure}. See the help file for
these functions for examples of how to explore and test the
functions. An alternative way of testing bin structure would be to
setup different models that have the bin structures desired.

\subsection{Sampling indices}\label{sec:indices}
Compared to sampling from the age and length compositions the indices
are relatively straightforward. The OM \texttt{.dat} file contains the
annual biomass values, and the \texttt{change\_index} function uses
these values as the expected value for all fleets (CPUE from fisheries
as well as scientific surveys). The function uses a bias adjusted
log-normal distribution with expected values given by the OM biomass
and a user-provided standard deviation term that controls the level of
variance.

More specifically, let
\begin{align*}
  B_y&=\text{the true (OM) biomass in year $y$}\\
  \sigma_y&=\text{the standard deviation provided by the user}\\
  X\sim N(0, \sigma_y)&=\text{a random normal variable}
\end{align*}
Then the sampled value, $B_y^\text{obs}$ is
\begin{equation*}
  B_y^\text{obs}=B_y e^{X-\sigma_y^2/2}
\end{equation*}
which has expected value of $B_y$ due to the bias adjustment term
$\sigma_y^2/2$. This process generates log-normal values centered at
the true value. It is possible for the user to specify the amount of
uncertainty (e.g.\ to mimic the amount of survey effort), but it is not
possible to induce bias in this process. The user-supplied $\sigma$
terms are written to the \texttt{.dat} file along with the sampled
values, meaning that the EM has the correct level of uncertainty. Thus
for all fleets the EM has unbiased estimates of $B_y$ and the true
$\sigma_y$.

\section{Incorporating process error}\label{sec:pro-error}
Process error is incorporated into the OM in the form of deviates in
recruitment (recdevs) from the stock-recruit relationship. Unlike the
observation error, the process error affects the population dynamics
and thus must be done before running the OM. The \texttt{ss3sim}
package contains a built-in set of recruitment deviations for up to
150 years and 500 iterations that are used if none are passed by the
user. The columns in the matrix represent iterations of the
simulation, and the rows represent years. These built-in recruitment
deviations are standard normal deviates and are multiplied by
$\sigma_r$ (recruitment standard deviation) as specified in the OM,
and bias adjusted within the package.That is,
\begin{equation*}
  \text{recdev}_i=\sigma_r z_i-\sigma_r^2/2
\end{equation*}
where $z_i$ is a standard normal deviate and the bias adjustment term
($\sigma_r^2/2$) makes the deviates have an expected value of 0 after
exponentiation.

If are not specified, then the package will use these built-in
recruitment deviations.  Alternative you can specify your own
recruitment deviations, via the argument \texttt{user\_recdevs} to the
top-level function \texttt{ss3sim\_base}. Ensure that you pass a matrix
with at least enough columns (iterations) and rows (years). The
user-supplied recruitment deviations are used exactly as specified
(i.e.\ not multiplied by $\sigma_r$ as specified in the SS model),
and\textbf{ it is up to you to bias correct them manually} by
subtracting $\sigma_r^2 / 2$ as is done above. This functionality
allows for flexibility in how the recruitment deviations are
specified, for example running deterministic runs (Section
\ref{sec:deterministic}).

Note that for both built-in and user-specified recdevs \texttt{ss3sim}
will reuse the same set of recruitment deviations for all iterations
across scenarios. For example if you have two scenarios and run 100
iterations of each, the same set of recruitment deviations are used
between these two scenarios.

\section{Stochastic reproducibility}\label{sec:reproduce}

In many cases, you may want to make
the observation and process error reproducible.
For instance, you may want to reuse process error
so that differences between scenarios
are not confounded with process error.
More broadly, you may want to make a simulation reproducible
on another machine by another user (such as a reviewer).

You can make the observation error reproducible by setting the
\texttt{seed} argument in the top level functions,
e.g.\ \texttt{ss3sim\_base}.  The \texttt{seed} argument takes a vector
of seed values at least as long as the number of iterations you are
running in your simulation and reuses them across different
scenarios. If \texttt{seed} is not set then no seed is used.

The process error (recruitment deviations) are reused across scenarios
and thus are by default reproducible for built-in and user-specified
values. If you want the different scenarios to have different process
error you will need to make separate calls to \texttt{run\_ss3sim} for
each scenario and pass a different matrix of \texttt{user\_recdevs}
(see Section \ref{sec:pro-error}). For most applications this should
not be necessary.

\bibliographystyle{apalike}
\bibliography{refs}

\clearpage

\appendix
\appendixpage
\addappheadtotoc

\section{Setting up a new operating model}
\label{sec:om-setup}
In some cases the user may wish to adapt their own \texttt{SS3} model to work
with the \texttt{ss3sim} package. This is possible but may be difficult because
the functions in \texttt{ss3sim} were developed to work with these three models and
a model with a different structure may cause errors in these functions. For
instance, the \texttt{change\_index} function does not have the capability to
handle more than one season (it could, but currently not developed to). Given
the myriad options available in \texttt{SS3} it is extremely difficult to write
auxilliary functions that will interact reliably with all combinations of these
options. For this reason, we recommend that users strongly consider trying to
modify an existing model rather than creating a new one
(Section \ref{sec:modify}). It is likely that the \texttt{ss3sim} functions
will need to be modified in some way in the process, so the user should be
familiar with both \texttt{SS3} and R.

The main purpose of the OM is to generate data files that can be read into the
EM. Thus the user needs to setup the \texttt{.dat} files in the OM such that
they conform to the structure needed by the EM. Two key examples are with survey
and age/length composition data. For the surveys (CPUE and scientific) the OM
\texttt{.dat} file will determine which years are available to the sampling
function \texttt{change\_index}, so if the year $y$ is desired in the EM it needs
to be in the OM. In practice it may be easiest to just include all years in the
OM if suveys will be dynamic (i.e.\ changing years between scenarios), or if it
will be fixed for all scenarios to set it to match the EM exactly.

Similarly with age/length compositions the OM \texttt{.dat} file will determine
which years and bins are available to the sampling functions
\texttt{change\_agecomp} and \texttt{change\_lcomp}. If dynamic binning is to be
used, the user should setup the \texttt{.dat} file so that all desired
combinations of bins are possible (see Section \ref{sec:dybin} for more
details). Specifically, the user must specify small enough bins (no smaller than
the population bin specified in the appropriate section in the OM \texttt{.dat} file)
OM \texttt{.ctl} bins) so that they can easily be re-binned. Alternatively,
if composition data is not to be explored in the simulation then the user can
just set the OM \texttt{.dat} file to match the desired input for the EM
\texttt{.dat}.

For those users who choose to create a new \texttt{ss3sim} model, we outline the
steps to take an existing \texttt{SS3} model and modify it to work with the
\texttt{ss3sim} package.  First, we cover setting up an operating model and then
in Section \ref{sec:em-setup} we cover setting up an estimation model.

\subsection{Starter file modifications}
\begin{enumerate}
\item
  Use the \texttt{.par} file to inform model parameters. To do so change \\
  \texttt{\# 0=use init values in control file} to \texttt{1}.
  Parameter values specified in the \texttt{.ctl} file will now be ignored.
\item
  Generate detailed report files (containing age-structure information) by setting \\
  \texttt{\# detailed age-structured reports in REPORT.SSO} to \texttt{1}.
\item
  Generate data by setting\\
  \texttt{\# Number of datafiles to produce} to \texttt{3}. \\
  If \texttt{X=1} it only generates the original data
  If \texttt{X=2} it generates the original data and the expected value data (based on model specification)
  If \texttt{X>=3} it generates all the above and \texttt{X-2} bootstrapped data
\item
  Turn off parameter estimation by changing\\
  \texttt{\# Turn off estimation for parameters entering after this phase}\\
  to \texttt{0}. Turning off parameter estimation facilitates running the model in
  a deterministic fashion.
\item
  Turn off parameter jittering by setting\\
  \texttt{\# jitter initial parm value by this fraction} to \texttt{0}.
\item
  Turn off retrospective analyses by setting\\
  \texttt{\# retrospective year relative to end year}\\
  to \texttt{0}. To analyze the data for retrospective patterns,
  use the \texttt{R} case file.
\item
  Specify how catch is reported by setting \texttt{\# F\_report\_units}
  to 1 if catch is reported in biomass or \texttt{2} if catch is reported in
  numbers. Additionally, comment out the next line,
  \texttt{\#\_min and max age over which average F will be calculated},
  by removing all characters prior to the hash symbol.
\item
  Implement catches using instantaneous fishing mortality by changing\\
  \texttt{\# F\_report\_basis} to \texttt{0}.
\end{enumerate}

\subsection{Control file modifications}
\begin{enumerate}
\item
  Specify all environmental deviates
  to be unconstrained by bounds by setting
  \texttt{\#\_env/block/dev\_adjust\_method} to \texttt{1}.
  If the method is set to \texttt{2},
  parameters adjusted using environmental covariate inputs
  will be adjusted using a logistic transformation to ensure that
  the adjusted parameter will stay within the bounds of the base parameter.
\item
  Turn on recruitment deviations by specifying
   \texttt{\#do\_recdev} to \texttt{1}.
   Using the next two lines, specify the use of recruitment deviations
   to begin and end with the start and end years of the model.
\item Turn on additional advanced options for the recruitment deviations
   by specifying \texttt{\# (0/1) to read 13 advanced options} to \texttt{1}.
\item Set \texttt{\#\_recdev\_early\_start} to \texttt{0} so that
  the model will use the\\ \texttt{\# first year of main recr\_devs}.
\item Set \texttt{\#\_lambda for Fcast\_rec\_like occurring before endyr+1}
   to \texttt{1}. This lambda is for the log likelihood of the
   forecast recruitment deviations that occur before the first year of forecasting.
   Values larger than one accommodate noisy data at the end of the time series.
\item Recruitment is log-normally distributed in SS.
   If inputting a normally distributed recruitment deviations specify\\
   \texttt{\#\_max\_bias\_adj\_in\_MPD} to \texttt{-1}
   so that SS performs the bias correction for you.
   If inputting bias corrected normal recruitment deviation, specify it at \texttt{0}.
   Either method will lead to the same end result.
\item Use any negative value in line \texttt{\# F ballpark year},
   to disable the use of a ballpark year to determine fishing mortality levels.
\item Specify \texttt{\# F\_Method} to \texttt{2},
   which facilitates the use of a vector of instantaneous fishing mortality levels.
   The max harvest rate in the subsequent line will depend upon
   the fishing mortality levels in your simulation.
   Following the max harvest rate, specify a line with three value separated by
   spaces. The first value is the overall start F value, followed by the phase.
   The last value is the number of inputs.
   Set the number of inputs to \texttt{1},
   because the actual fishing mortality trajectory
   will be specified in the \texttt{.dat} file.
   Next, specify a single line with six values, separated by spaces,
   where the values correspond to fleet number, start year, season, fishing
   mortality level, the standard error of the fishing mortality level, and a negative
   phase value. E.g \texttt{1 2000 1 0 0.01 -1}
\item
  If needed, change the specification of the \texttt{.ctl}
  using the functions available in the \texttt{ss3sim} package.
  E.g \texttt{change\_growth}, \texttt{change\_sel}.
\end{enumerate}

\subsection{Data file modifications}
\begin{enumerate}
\item Specify the start and end year for the simulation by modifying
   \texttt{\#\_styr} and \texttt{\#\_endyr}.
   Years can be specified as a number line
   (i.e.\ \texttt{1}, \texttt{2}, \texttt{3}, \ldots) or
   as actual years
   (i.e.\ \texttt{1999}, \texttt{2000}, \texttt{2001}, \ldots)
\item Specify the names for each fleet in an uncommented line
   after the line \texttt{\#\_N\_areas}.
   Names must be separated by a \texttt{\%} with no spaces.
   It is these names which you will use in the plain text case files
   to specify and change characteristics of each fleet throughout the simulation.
   E.g \texttt{Fishery\%Survey1\%Survey2}
\item Specify the mean body weight across all selected sizes and ages
   to be specific to measured fish by setting \texttt{\#N observations}
   to \texttt{0}. Subsequently, specify \texttt{1} degree of freedom for the
   Student's T distribution used to evaluated the mean body weight deviations
   in the following line. The degrees of freedom must be specified
   even if there are zero mean body weight observations.
\item Set the length bin method to 1 or 2 in the line labelled
   \texttt{\# length bin method}.
   Using a value of \texttt{1}, the bins refer to the data bins (specified later).
   Using a value of \texttt{2} instructs SS to generate the binwidths from a user specified
   minimum and maximum value.
   In the following three lines, specify the binwidth for population size composition data;
   the minimum size, or the lower edge of the first bin and size at age zero;
   and the maximum size, or lower edge of the last bin.
   The length data bins MUST be wider than the population bin,
   but the boundaries do not have to align.
\item Specify \texttt{\#\_comp\_tail\_compression} to any negative value
   to turn off tail compression.
\item Specify \texttt{\#\_add\_to\_comp} to a very small number E.g \texttt{1e-005}.
   This specifies the value that will be added to each composition (age and length) data bins
\item Set the length bin range method for the age composition data
  (used when the conditional age at length data exists)
   to 1, 2 or 3 in the line \texttt{\#\_Lbin\_method}
   depending on the data you have or the purpose of the study
\end{enumerate}

\subsection{Testing the new operating model}
After completing the above steps, check that the \texttt{SS3} model is
functional by running a single iteration of the model and verifying that the
data are read in correctly and expected values of the population dynamics are
written to the \texttt{.dat} files (and sensical). We also advise manually
testing the \texttt{ss3sim} \texttt{R} functions that manipulate the OM (e.g.,
\texttt{change\_tv}) and check that the model still runs correctly after this
manipulation. The help files for the functions demonstrate how to use the
functions to test models. Note that the OM will not be a valid \texttt{SS3}
model in the sense that ADMB cannot run and produce maximum likelihood
estimates of parameters; it is intended to only be run for one
iteration to generate the population dynamics using the starting values
specified in the \texttt{.ctl} file.

\section{Setting up a new estimation model}
\label{sec:em-setup}
Unlike the OM, the EM needs to be a valid \texttt{SS3} model and run to achieve MLE
estimates (and possibly standard errors). Thus the OM needs to be adapted to
create a new EM.

\subsection{Starter file modifications}
\begin{enumerate}
\item Change the names of the \texttt{.dat} and \texttt{.ctl} files
   to your chosen naming scheme.
\item Specify the model to use parameter values found in the \texttt{.ctl} file,
   by changing \texttt{\# 0=use init values in control file; 1=use ss3.par}
   to \texttt{0}.
\item Turn on parameter estimation by changing\\
   \texttt{\# Turn off estimation for parameters entering after this phase}
   to a value larger than the max phase specified in the \texttt{.ctl} file.
\end{enumerate}

\subsection{Control file modifications}
\begin{enumerate}
\item Set the phases of the parameters to positive or negative value to
   inform SS to estimate or fix the parameters, respectively.
\item Set the \texttt{\#\_recdev phase}
   to a positive value to estimate yearly recruitment deviations.
\item If using bias adjustment set \texttt{\#\_recdev\_early\_phase}
   to a positive value.
   Estimates for the years and maximum bias adjustment
   can initially be inputted with approximations
   or use the bias adjustment function within \texttt{ss3sim}
   to find appropriate values for the base case EM
   and input them in the appropriate lines.
\item Specify \texttt{\# F\_Method} to \texttt{3},
   which allows the model to use catches to estimate
   appropriate fishing mortality levels.
   The max harvest rate in the subsequent line will depend upon
   the fishing mortality levels in your simulation.
   An additional line must be inserted after the maximum harvest rate
   to specify the number of iterations used in the hybrid method.
   Specify the line as follows
   \texttt{3 \# F\_Method:  1=Pope; 2=instan. F; 3=hybrid (hybrid is recommended)},
   where the number can range from \texttt{3} to \texttt{7}.
 \item Use the functions in the \texttt{ss3sim} package
   to change the estimation specification in the EM.
   E.g \texttt{change\_e}
\end{enumerate}

\subsection{Data file modifications}
The \texttt{data.ss\_new} files produced when executing the OM contain the
expected values of the OM population dynamics. The data the EM model is fit to
needs to be sampled with observation error from these expected values in order
to mimic the random sampling process done with real fisheries data. The
\texttt{ss3sim} package provides three functions which carry out the random
sampling process and generate \texttt{.dat} files to be used in the EM. See
Section \ref{sec:obs-error} for more details.

\subsection{Testing the new estimation model}
After completing the above steps run the model manually and verify that it
loads the data properly and the objective. If it works correctly, try running
deterministic cases on the model (Section \ref{sec:deterministic}) and further
verify that \texttt{ss3sim} functions that modify the EM (e.g.,
\texttt{change\_e}) act correctly on the model. It is possible that some of the
functions will not work perfectly with the new models. In this case, it may be
necessary to modify the \texttt{ss3sim} functions to be compatible with the new
OM and EM.

\end{document}
